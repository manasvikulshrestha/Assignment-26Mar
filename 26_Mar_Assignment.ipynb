{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc31300-8a1b-440d-a602-737e2de730b2",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303f4fc-c5c6-40de-a44e-99eca87dbff5",
   "metadata": {},
   "source": [
    "Q1. Simple linear regression involves predicting a continuous dependent variable using a single independent variable. For example, predicting a student's exam score based on the number of hours they studied. Multiple linear regression extends this concept to include two or more independent variables in predicting the dependent variable. An example could be predicting house prices based on factors like area, number of bedrooms, and location.\n",
    "\n",
    "Q2. Linear regression makes several assumptions: linearity, independence, homoscedasticity, and normality of residuals. These assumptions can be checked through diagnostic plots like residual plots, QQ plots, and tests like the Shapiro-Wilk test for normality and Durbin-Watson test for autocorrelation.\n",
    "\n",
    "Q3. In a linear regression model \\( Y = \\beta_0 + \\beta_1X + \\epsilon \\), the slope (\\( \\beta_1 \\)) represents the change in the dependent variable for a one-unit change in the independent variable, while the intercept (\\( \\beta_0 \\)) represents the value of the dependent variable when the independent variable is zero. For example, in a study relating hours of study (X) to exam scores (Y), the slope would represent the change in exam score for each additional hour studied, and the intercept would represent the expected exam score when the student studies zero hours.\n",
    "\n",
    "Q4. Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It works by iteratively adjusting model parameters in the direction of steepest descent of the cost function until convergence. This helps in finding the optimal model parameters that best fit the data.\n",
    "\n",
    "Q5. Multiple linear regression extends simple linear regression by allowing for more than one independent variable to predict the dependent variable. Instead of a straight line, it fits a hyperplane to the data, where each independent variable has its own coefficient.\n",
    "\n",
    "Q6. Multicollinearity occurs when independent variables in a multiple linear regression model are highly correlated with each other. This can cause issues with interpreting individual coefficients and can inflate standard errors. It can be detected using methods like correlation matrices or variance inflation factor (VIF), and addressed by removing one of the correlated variables or using regularization techniques like ridge regression.\n",
    "\n",
    "Q7. Polynomial regression fits a curve to the data instead of a straight line, allowing for more complex relationships between the independent and dependent variables. Unlike linear regression, which fits a first-degree polynomial (straight line), polynomial regression can fit higher-degree polynomials.\n",
    "\n",
    "Q8. Advantages of polynomial regression include its ability to model non-linear relationships and capture more complex patterns in the data. However, it can lead to overfitting and may be computationally expensive with higher-degree polynomials. Polynomial regression is preferred when the relationship between variables is non-linear and simple linear regression is insufficient to capture it effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
